# SLURM settings
export ACCOUNT=${ACCOUNT:-"AIFAC_L01_028"}
export PARTITION=${PARTITION:-"boost_usr_prod"}
export RUN_NAME=${RUN_NAME:-"moe_speed_run"}
export CONTAINER_MOUNTS="${MEGATRON_PATH},${WORK},${FAST},${MEGATRON_CACHE_FOLDER},${HF_HOME}"
export DATASET="nemotron-cc-hq"
export OUTPUT_FOLDER="moe_speed_runs"
export HF_HUB_OFFLINE=1
#export QOS="boost_qos_dbg"
export QOS="normal"

# Output path
export OUTPUT_PATH=${OUTPUT_PATH:-"${FAST}/${OUTPUT_FOLDER}/${MODEL}-TP${TP}PP${PP}EP${EP}VPP${VPP}-MBS${MBS}GBS${GBS}"}

# Data paths

VOCAB_FILE="/leonardo_work/EUHPC_E03_068/shared/models/EleutherAI/gpt-neox-20b/vocab.json"
MERGE_FILE="/leonardo_work/EUHPC_E03_068/shared/models/EleutherAI/gpt-neox-20b/merges.txt"

declare -A DATA_PATHS=(
    # DATASET:MODEL:                DATA_PATH
    ["nemotron-cc-hq:Mixtral-8x2B"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX/merged_0"
    ["nemotron-cc-hq:Mixtral-8x7B"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX"
    ["nemotron-cc-hq:Mixtral-8x22B"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX"
    ["nemotron-cc-hq:DeepSeek-V2"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX"
    ["nemotron-cc-hq:DeepSeek-V2-Lite"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX/merged_0"
    ["nemotron-cc-hq:DeepSeek-V2-SuperLite"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX/merged_0"
    ["nemotron-cc-hq:custom_model_dalton"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX/merged_0"
    ["nemotron-cc-hq:custom_model_dense"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX/merged_0"
    ["nemotron-cc-hq:DeepSeek-V3"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX"
    ["nemotron-cc-hq:Qwen2-57B-A14B"]="$WORK/datasets/Nemotron-cc-2024/tokenized/quality-high_kind-actual/GPT-NeoX"
)

# Tokenizer models
declare -A TOKENIZER_MODELS=(
    # DATASET:MODEL:  TOKENIZER_MODEL
    ["Mixtral-8x2B"]="EleutherAI/gpt-neox-20b"
    ["Mixtral-8x7B"]="EleutherAI/gpt-neox-20b"
    ["Mixtral-8x22B"]="EleutherAI/gpt-neox-20b"
)

# Checkpoint paths
declare -A CHECKPOINT_PATHS=(
    # MODEL:         CHECKPOINT_PATH
    ["Mixtral-8x7B"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["Mixtral-8x22B"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["DeepSeek-V2"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["DeepSeek-V2-Lite"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["DeepSeek-V2-SuperLite"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["custom_model_dalton"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["custom_model_dense"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["DeepSeek-V3"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
    ["Qwen2-57B-A14B"]="${FAST}/${OUTPUT_FOLDER}/checkpoints"
)

# Function to get path based on MODEL
get_path_based_on_model() {
    local -n paths=$1
    local model_name=$2
    # Verify if model_name exists in paths array
    if [[ -v paths[$model_name] ]]; then
        echo "${paths[$model_name]}"
    else
        echo ""
    fi
}

# Set DATA_PATH based on MODEL
export DATA_PATH=${DATA_PATH:-${DATA_PATHS["${DATASET}:${MODEL}"]}}

# Set TOKENIZER_MODEL based on MODEL
export TOKENIZER_MODEL=$(get_path_based_on_model "TOKENIZER_MODELS" "${MODEL}")

# Set LOAD_PATH based on MODEL
if [[ ${PRETRAIN} == 0 ]]; then
    export LOAD_PATH=$(get_path_based_on_model "CHECKPOINT_PATHS" "${MODEL}")
fi
